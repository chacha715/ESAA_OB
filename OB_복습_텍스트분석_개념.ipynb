{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn+RAozSXOgfoCCwKCET9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha715/ESAA_OB/blob/main/OB_%EB%B3%B5%EC%8A%B5_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D_%EA%B0%9C%EB%85%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ch.8 텍스트 분석**\n",
        "\n",
        "\n",
        "**NLP냐 텍스트 분석이냐**\n",
        "\n",
        ": NLP는 머신이 인간의 언어를 이해하고 해석하는데 중점을 두고 기술이 발전해 왔으며, 텍스트 마이닝은 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점을 둔 것\n",
        "\n",
        "\n",
        "- NLP는 언어를 해석하기 위한 기계번역, 자동으로 질문을 해석하고 답을 해주는 질의응답 시스템 등의 영역 등에서 텍스트 분석과 차별점이 존재. 이 기술이 발전함에 따라 텍스트 분석도 같이 발전하고 발전 가능해짐\n",
        "\n",
        "- 머신러닝, 언어이해, 통계 등을 활용해 모델을 수립하고 정보를 추출해 비즈니스 인텔리전스나 예측 분석등의 분석 작업을 주로 수행. 머신러닝 기술에 힘입어 텍스트분석은 크게 발전하고 있으며 주로\n",
        "\n",
        " *   텍스트분류: 문서가 특정 분류 혹은 카테고리에 속하는 것을 예측하는 기법을 통칭하는 것으로 기사의 정치, 연예, 사회, 문화 카테고리를 정하거나 스팸메일 검출 프로그램등이 해당됨\n",
        "\n",
        " *   감성 분석: 텍스트에서 나타내는 감정, 판단, 믿음, 의견, 기분 등의 주관적인 요소를 분석하는 기법의 총칭으로 소셜미디어 감정분석, 영화나 제품에 대한 긍정 또는 리뷰, 여론조사 등에서 다양하게 활용됨\n",
        "\n",
        " *   텍스트 군집화: 비슷한 유형의 문서에 대해 군집화를 수행하는 기법으로 텍스트 분류를 비지도학습으로 수행하는 방법이며 유사도 측정으로 이를 수행 가능\n",
        "\n",
        " *   텍스트 요약: 텍스트 내에서 중요한 주제나 중심사상을 추출하는 기법으로 대표적으로 토픽 모델링이 있음\n",
        "\n",
        "  등이 있음"
      ],
      "metadata": {
        "id": "8JIAh2gT1FxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **01. 텍스트 분석 이해**\n",
        "------\n",
        "\n",
        "*   비정형 데이터인 텍스트를 분석하는 것으로 대부분의 머신러닝 모델은 주어진 정형데이터 기반에서 모델을 수립하고 예측을 수행했으며 숫자형의 피처기반 데이터만 입력 가능하도록 했기 때문에 이것을 피처형태로 어떻게 추출하며 추출된 피처에 의미있는 값을 부여하는 것이 매우 중요한 요소임\n",
        "\n",
        "*   이렇게 추출된 텍스트는 단어의 조합인 벡터값으로 표현될 수 있는데 이를 피처 벡터화 혹은 피처추출이라 하며, 이를 위한 방법에는 BOW와 word2voc 방법이 있음\n",
        "\n",
        "\n",
        "\n",
        "**텍스트 분석 수행 프로세스**\n",
        "\n",
        "1. 텍스트 사전 준비작업(텍스트 전처리): 텍스트를 피처로 만들기 전에 미리 클렌징, 대소문자 변경, 특수문자 제거 등의 작업과 단어 토큰화작업, 의미없는 단어제거, 어근 추출등의 정규화작업을 수행\n",
        "\n",
        "2. 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터값을 할당. BOWDHK WORD2VEC이 있으며 BOW에는 count기반과 tf-idf기반벡터화가 있음\n",
        "\n",
        "3. ML 모델 수립 및 학습 예측 평가\n",
        "\n",
        "\n",
        "\n",
        "**파이썬 기반 NLP, 텍스트 분석 패키지**\n",
        "\n",
        "- 대표적인 패키지 NTLK는 방대한 데이터와 서브모듈, 다양한 데이터세트를 지원하나 성능과 정확도 등에서 부족한 부분이 있어 Genism, SpaCy가 이를 보완하며 실 업무에서 자주 활용됨\n",
        "\n",
        " *   NLTK: 방대한 데이터세트와 서브모듈을 가지며 거의 모든 영역을 커버하나 수행속도 면에서 아쉬운 점이 존재\n",
        "\n",
        " *   Genism: 토픽 모델링 분야에서 가장 두각을 나타내는 패키지로 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공해왔으며 word2vec등의 신기능을 제공하고 많이 사용됨\n",
        "\n",
        " *   SpaCy: 뛰어난 수행성능의 NLP 패키지\n",
        "\n",
        "- 다만 어근처리에 대한 라이브러리는 없으나 텍스트를 일정수준으로 가공하고 머신러닝 알고리즘에 텍스트 데이터를 피처로 처리하기 위한 편리한 기능을 제공하고 있어 사이킷런으로도 텍스트 분석을 충분히 수행 가능"
      ],
      "metadata": {
        "id": "ucCSax0_5dsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **02. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**\n",
        "------\n",
        "\n",
        ": 텍스트 자체를 바로 피처화할 수 없어 가공 준비가 필요한데 먼저 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 'nlp' 어플리케이션에 입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 사전작업을 수행하는 것으로 정규화 과정은 크게\n",
        "\n",
        "*   클렌징: 방해되는 불필요한 문자, 기호 등을 사전에 제거\n",
        "\n",
        "*   토큰화: 문서에서 문장을 분리하는 문장 토큰화와 단어를 토큰으로 분리하는 단어 토큰화로 분류\n",
        "\n",
        "  로 나누어짐"
      ],
      "metadata": {
        "id": "lXen2kd_5hTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**단어 토큰화**\n",
        "\n",
        "*   문장을 단어로 토큰화하는 것으로 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하면 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행 가능\n",
        "\n",
        "*   마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화할 수 있으므로 단어의 순서가 중요하지 않은 경우 단어토큰화만 해도 괜찮다"
      ],
      "metadata": {
        "id": "GLesi0Rl1Fc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스톱워드 제거**\n",
        "\n",
        "- 필터링/스톱 워드 제거/철자 수정\n",
        "\n",
        "*   이 중 스톱워드 제거는 분석에 큰 의미가 없는 스톱워드를 제거하는 것으로 영어에서 is, the, a, will과 같이 필수 문법 요소이나 큰 의미가 없는 단어를 제거하지 않으면 오히려 빈도로 인해 중요단어로 등장할 수 있어 중요한 전처리 작업임\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CB3yltZT8-8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stemming과 lemmatization**\n",
        "\n",
        "*   둘다 문법적으로 또는 의미적으로 변화하는 단어의 원형을 찾기 위한 것으로 lem은 stem에 비해 의미론적인 기반에서 단어의 원형을 찾으며 stem은 원형 단어로 변환시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 단어에서 일부 철자가 훼손된 어근 단어를 추출. 반면 lem은 품사와 같은 문법적 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아주기 때문에 그만큼 더 오래 걸림\n",
        "\n",
        "*   NLTK는 다양한 stemmer를 제공. 이는 진행형, 3인칭 단수, 과거형에 따른 동사, 비교 등 단순하게 원형 단어를 찾아줌"
      ],
      "metadata": {
        "id": "SJTj02lR_8sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **03. Bag of WOrds - BOW**\n",
        "------\n",
        "\n",
        "*   문서가 갖는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여해 피처 값을 추출하는 모델로 모든 단어를 한꺼번에 봉투(bag)에 넣어 흔들어 섞는다는 의미로 bow라 부름\n",
        "\n",
        " *   문장 1:\n",
        "My wife likes ot watch baseball games and my daughter likes towatch baseball games too\n",
        " *   문장 2:\n",
        "My wife likes to play baseball\n",
        "\n",
        "   라는 문장이 있을 때, bow의 단어수 기반으로 피처를 추출하게 된다면,\n",
        "\n",
        "\n",
        "1. 문장 1과 문장 2에있는 모든 단어에서 중복을 제거하고 각 단어를 칼럼 형태로 나열한 뒤 각 단어에 고유 인덱스를 부여\n",
        "\n",
        "2. 개별 문장에서 해당 단어가 나타나는 횟수를 각 단어에 기재\n",
        "\n",
        "\n",
        "*   bow 모델의 장점은 쉽고 빠른 구축에 있으며 문서의 특징을 잘 나타낸다는 부분이나, 문맥의미 반영 부족과 희소행렬 문제가 있음\n",
        "\n",
        " *   문맥의미 반영 부족: 단어 순서를 고려하지 않아 문장내의 단어의 문맥 의미가 무시되며 이를 보완하기 위해 n_gram을 활용할 수 있으나 제한적임\n",
        "\n",
        " *   bow로 피처벡터화를 수행하면 최소행렬형태의 데이터세트가 만들어지지 않아 문서마다 서로 다른 단어로 구성되기에 단어가 문서마다 나타나지 않는 경우가 훨씬 더 많음. 즉 대부분의 데이터는 0값으로 채워지는데 이러한 것을 희소행렬이라 하고 이는 알고리즘의 수행시간과 예측성능을 악화시킨다는 단점이 있음\n",
        "\n",
        "\n",
        "\n",
        "**BOW 피처 벡터화**\n",
        "\n",
        "*   머신러닝 알고리즘은 일반적으로 숫자형 피처를 데이터로 입력받아 동작하기 때문에 텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수 없어 특정 의미를 갖는 숫자형 값인 벡터값으로 변환해야 하는데 이를 피처벡터화라 함\n",
        "\n",
        " *   예를 들어 각 문서의텍스트를 단어로 추출해 피처로 할당하고, 각 단어의 발생 빈도와 같은 값을 이 피처에 값으로 부여해 각 문서를 이 단어 피처의 발생빈도 값으로 구성된 벡터로 생성하는 기법\n",
        "\n",
        "\n",
        "*   피처 벡터화는 기존 텍스트 데이터를 또 다른 형태의 피처조합으로 변경하기 때문에 넓은 범위의 피처추출에 포함하며 이는 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
        "\n",
        "\n",
        "*   일반적으로 카운트기반의 벡터화, tf-idf기반의 벡터화라는 두가지 방식을 가짐. 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수, 즉 카운트를 부여하는 경우를 카운트 벡터화로 카운트값이 높을수록 중요단어로 인식됨. 그러나 카운트만 부여할경우 단순 횟수세기로 인해 빈도수만 따지게 되어 언어의 특성상 문장에 자주 사용되는 단어가 중요단어로 등극할 수 있기에 이를 보완하려 tf-idf를 사용. 개별 문서에서 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 나타나는 단어에 대해 페널티를 주는 것\n",
        "\n",
        "\n",
        "*   어떤 문서의 특정 단어가 자주 나타나면 중요 단어일 수 있으나 다른 문서에도 나타난다면 그냥 언어 특성상 자주 나타나는 단어일 확률이 높음. 이러한 단어의 등장횟수로만 중요도를 평가하면 문제가 되기 때문에 모든 문서의 반복적 단어에 대해 페널티를 부여하는 것\n",
        "\n",
        "\n",
        "\n",
        "**사이킷런의 count 및 TF-IDF 벡터화 구현: countVectorizer, TfidfVectorizer**\n",
        "\n",
        ": 사이킷런의 countvectorizer 클래스는 카운트 기반의 벡터화를 구현한 클래스로 피처벡터화만 수행하는 것이 아니라 소문자 일괄변환, 토큰화, 스톱워드 필터링 등 텍스트 전처리도 동시에 수행. 마찬가지로 fit과 transform으로 피처벡터화된 객체를 반환\n",
        "\n",
        "*   max_df: 전체 문서에 걸쳐 높은 빈도수인 단어를 필터링하기 위한 파라미터로 정수값을 가지면 그 정수값 이하로 나타나는 단어만 피처로 추출하고 소수점인 경우 전체문서에 걸쳐 빈도수가 nn퍼센트 까지의 단어만 피처로 추출하고 나머지 상위 n퍼센트는 피처로 추출하지 않음\n",
        "\n",
        "*    min_df: 전체 문서에 걸쳐 너무 낮은 빈도수의 단어를 제외하기 위한 파라미터로 너무 낮은 파라미터는 가비지 단어이거나 중요하지 않을 확률이 큼. max_df와 동일하다.\n",
        "\n",
        "*   max_features: 추출 피처의 개수를 제한하며 정수로 값을 지정하여 가장 높은 빈도 순으로 상위 정수개의 피처를 추출\n",
        "\n",
        "*   stopwords: 영어로 지정하면 영어의 스톱워드로 지정된 단어는 추출에서 제외됨\n",
        "\n",
        "*   n_gram_range: 단어 순서를 보강하기 위한 것으로 토큰화된 단어를 순서대로 묶어 피처로 추출\n",
        "\n",
        "*   analyzer: 피처 추출을 수행한 단위를 지정. 기본은 word이며 character의 특정 범위를 피처로 만드는 등 특정경우에 적용\n",
        "\n",
        "*   token_pattern: 토큰화를 수행하는 정규 표현식 패턴을 지정하며 공백이나 개행문자 등 구분된 단어분리자 사이의 2문자 이상의 단어를 토큰으로 분리\n",
        "\n",
        "*   tokenizer: 토큰화를 별도커스텀 함수로 이용 시 적용하며 일반적으로 counttokenizer 클래스에서 변환시 수행하는 별도 함수를 tokenizer 파라미터에 적용하면 됨\n",
        "\n",
        "\n",
        " 크게 사전데이터 가공(소문자변환)> n_gram_range 반영의 토큰화(디폴트는 anlayzer = true) > 텍스트 정규화(stop words 필터링 수행 ) > 피처 벡터화 (max_df, min_df 등 반영 )로 나뉨\n",
        "\n",
        "\n",
        "\n",
        "**BOW 벡터화를 위한 희소 행렬**\n",
        "\n",
        "*   사이킷런을 이용해 텍스트를 피처단위로 벡터화해 변환하고 csr 형태의 희소행렬을 반환. 사용자 입장에서 피처벡터화된 희소행렬이 어떤 형태인지 중요하진 않을수 있으나, 난이도 있는 ml모델 수립을 위해서는 희소행렬의 형태파악이 필요\n",
        "\n",
        "*   모든 문서의 단어를 추출해 피처로 벡터화하는 방법은 필연적으로 많은 피처칼럼을 만들 수밖에 없으며, 모든 문서의 단어 중복을 제거하고 피처로 만들면 일반적으로 매우 많은 단어가 생성됨. 이런 대규모 행렬이 생성되더라도 레코드의 각 문서가 가지는 단어의 수는 제한적이기 때문에 행렬의 대부분 값은 0일 수밖에 없음. 이런 대부분의 값이 0인 행렬을 희소행렬이라 함\n",
        "\n",
        "*   이는 너무 많은 불필요한 0값이 메모리 공간에 할당되어 메모리 공간을 많이 차지하며 행렬 크기가 커 연산시에도 시간이 많이 소모되기에 이를 변환할 필요가 있는데 그 대표적인 방법으로 coo, csr이 있음\n",
        "\n",
        "\n",
        "\n",
        "**희소 행렬 - COO 형식**\n",
        "\n",
        ": 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식으로 scipy를 이용하면 할 수 있음"
      ],
      "metadata": {
        "id": "u1YJG9ctCza-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**희소 행렬 - CSR 형식**\n",
        "\n",
        ": coo배열에서 행위치 배열에도 순차적인 같은 값이 반복되는 것을 알 수 있는데, 행위치 배열이 0부터 순차적으로 증가한는 값으로 이뤄졌다는 특성을 고려하면 행 위치 배열의 고유한 값의 시작위치만 표기하는 방법으로 이러한 반복을 제거할 수 있음\n",
        "\n",
        "행 위치 배열 [0. 0, 1, 1. 1, 1, 1.2. 2. 3. 4. 4. 5] 를 csr로 변환하면[0, 2. 7. 9. 10. 12]가 되고, 맨 마지막에는 데이터의 총 항목 개수를 배열에 추가\n",
        "\n",
        "이는 사이파이의 csr_matrix를 통해 쉽게 할 수 있음"
      ],
      "metadata": {
        "id": "GbWm9IlXBcZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **05. 감성분석**\n",
        "------\n",
        "\n",
        "\n",
        "**감성 분석 소개**\n",
        "\n",
        "감성분석(Sentiment Analysis): 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법으로 소셜 미디어, 여론조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용됨\n",
        "\n",
        "- 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성 수치를 계산하는 방법으로 수행\n",
        "\n",
        "  + 긍정 감성 지수와 부정 감성 지수를 합산하여 긍정 또는 부정 감성을 결정\n",
        "\n",
        "- 지도학습의 경우 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습을 수행한 뒤 이를 기반으로 다른 데이터의 감성분석을 예측하는 방법으로 수행되며, 일반적인 텍스트 기반의 분류와 거의 동일함\n",
        "\n",
        "- 비지도학습의 경우, 'Lexicon'이라는 일종의 감성 어휘 사전을 이용"
      ],
      "metadata": {
        "id": "35v9XXADRoTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**비지도학습 기반 감성 분석 소개**\n",
        "- 비지도 감성 분석은 Lexicon을 기반으로 수행\n",
        "\n",
        "  + 지도 감성 분석은 데이터 세트가 레이블 값을 갖고 있지만, 대부분의 감성 분석용 데이터에는 레이블 값이 없음\n",
        "\n",
        "- Lexicon: 감성만을 분석하기 위해 지원하는 감성 어휘 사전(한국어 지원x)\n",
        "\n",
        "  + 긍정 감성 또는 부정 감성의 정도를 의미하는 수치를 가지고 있으며 이를 감성 지수라고 함\n",
        "\n",
        "  + 감성 지수는 단어의 위치나 주변 단어, 문맥, POS 등을 참고하여 결정\n",
        "\n",
        "- NLTK 패키지: 많은 서브 모듈을 갖고 있으며 그 중에 감성 사전인 Lexicon도 포함\n",
        "\n",
        "\n",
        "\n",
        "**여러 패키지**\n",
        "\n",
        "- WordNet: NLP 패키지에서 제공하는 방대한 영어 어휘 사전\n",
        "\n",
        "  + 시맨틱(문맥상 의미) 분석 제공\n",
        "  \n",
        "  + NLP 패키지에서는 시맨틱을 프로그램적으로 인터페이스할 수 있는 다양한 방법 제공\n",
        "\n",
        "- 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 시맨틱 정보를 제공하기 위해 각각의 품사로 구성된 개별 단어를 Synset이라는 개념을 이용해 표현\n",
        "\n",
        "  + Synset: 단순한 하나의 단어가 아닌 그 단어가 가지는 문맥, 시맨틱 정보를 제공하는 WordNet의 핵심 개념\n",
        "\n",
        "- 그러나 NLTK의 예측 성능이 그리 좋지 못해 다른 감성 사전을 일반적으로 사용함\n",
        "\n",
        "  + SentiWordNet: WordNet과 유사하게 감성 단어 전용의 WordNet 구현, 3가지 감성 점수(긍정/부정/객관성)을 통해 결정\n",
        "\n",
        "  + Vader: 소셜 미디어의 텍스트에 대한 감성 분석을 제공하기 위한 패키지, 대용량 텍스트 데이터에 용이\n",
        "\n",
        "  + Pattern: 예측 성능 측면에서 가장 주목받는 패키지"
      ],
      "metadata": {
        "id": "GO1UUiatZjQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER를 이용한 감성 분석**\n",
        "\n",
        "- VADER: 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon\n",
        "\n",
        "  + SentimentIntensitynalyzer 클래스 이용\n",
        "  \n",
        "  + NLTK패키지의 서브 모듈이나 단독 패키지로 제공"
      ],
      "metadata": {
        "id": "jtspH5dyo0_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **07 | 문서 군집화 소개**\n",
        "------\n",
        "\n",
        "\n",
        "**문서 군집화 개념**\n",
        "\n",
        "- 비슷한 텍스트 구성의 문서를 군집화 하는 것\n",
        "\n",
        "- 동일한 군집에 속하는 문서를 같은 카테고리 소속으로 분류할 수 있으므로, 앞에서 소개한 텍스트 분류 기반의 문서 분류와 유사\n",
        "\n",
        "- 하지만, 텍스트 분류 기반의 문서 분류는 사전에 결정 카테고리 값을 가진 학습 데이터 세트가 필요함\n",
        "\n",
        "- 문서 군집화는 학습 데이터 세트가 필요없는 비지도 학습\n"
      ],
      "metadata": {
        "id": "XG0BiVttJt5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**군집별 핵심 단어 추출하기**\n",
        "\n",
        "- 각 군집에 속한 문서는 핵심 단어를 주축으로 군집화 되어 각 군집을 구성하는 핵심 단어가 어떤 것이 있는지 확인해 볼 것\n",
        "\n",
        "- 각 군집을 구성하는 단어 피처가 군집의 중심을 기준으로 얼마나 가깝게 위치해있는지: clusters_centes_\n",
        "\n",
        "- clusters_centes_: 행(개별 군집), 열(개별 피처)"
      ],
      "metadata": {
        "id": "jJgzwLbO-c5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cluster_centers_ 속성값을 이용해 각 군집별 핵심 단어를 찾기**\n",
        "\n",
        "- ndarray의 argsort()[:, ::-1]을 이용해 배열 내 값이 큰 순으로 정렬된 위치 인덱스 값을 반환(큰 값을 가진 배열 내 위치 인덱스 값을 반환)\n",
        "\n",
        "- get_cluster_details() 함수를 생성: 위 처리 담당\n",
        ": 배열 내에서 가장 값이 큰 데이터의 위치 인덱스를 추출한 뒤, 해당 인덱스를 이용해 핵심 단어 이름과 그때의 상대 위치 값을 추출해 변수에 기록하고 반환"
      ],
      "metadata": {
        "id": "Ogk9TMEj_J8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "### **08 | 문서 유사도**\n",
        "------"
      ],
      "metadata": {
        "id": "hvcOlj6d5qHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**문서 유사도 측정 방법 - 코사인 유사도**\n",
        "\n",
        ": 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것\n",
        "\n",
        "- 두 벡터의 내적을 총 벡터 크기의 합으로 나눈 것\n",
        "\n",
        "- 즉, 내적 결과를 총 벡터 크기로 정규화(N2 Norm)한 것\n",
        "\n",
        "- 문서의 유사도 비교에 가장 많이 사용됨"
      ],
      "metadata": {
        "id": "ITNHtC1N5xzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**두 벡터의 사잇각**에 따른 상호 관계\n",
        "\n",
        "- 유사 벡터들\n",
        "\n",
        "- 관련성이 없는 벡터들\n",
        "\n",
        "- 반대 관계인 벡터들"
      ],
      "metadata": {
        "id": "P_F6ZuWdRzGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "###**09 | 한글 텍스트 처리**\n",
        "------"
      ],
      "metadata": {
        "id": "9JMWRKUk53xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**한글 NLP 처리의 어려움**\n",
        "\n",
        "- 원인: 띄어쓰기, 다양한 조사"
      ],
      "metadata": {
        "id": "KtvaQgpj6Gvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KoNLPy**\n",
        "\n",
        ": 파이썬의 대표적인 한글 형태소 패키지"
      ],
      "metadata": {
        "id": "e2Z1DWG46JNo"
      }
    }
  ]
}