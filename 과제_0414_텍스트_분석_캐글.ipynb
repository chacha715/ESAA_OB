{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha715/ESAA_OB/blob/main/%EA%B3%BC%EC%A0%9C_0414_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%BA%90%EA%B8%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zL-2F16sqDk"
      },
      "source": [
        "1) https://dacon.io/competitions/official/235670/codeshare/1840?page=undefined&dtype=recent&ptype=undefined\n",
        "\n",
        "​\n",
        "\n",
        "2) https://dacon.io/competitions/official/235670/codeshare/1823?page=2&dtype=recent\n",
        "\n",
        "​\n",
        "\n",
        "* 형태소 분석(Stemming) \n",
        "\n",
        " - 1-3. Mecab()은 오류 문제가 있어 제외하고 코드를 작성\n",
        "\n",
        "​\n",
        "\n",
        "3) https://dacon.io/competitions/official/235670/codeshare/1841?page=2&dtype=recent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_uYrs7ETWpp",
        "outputId": "026377ae-2607-48aa-f4ac-35604997a287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/ESAA/23-1/DATA/0414/\") # working directory 설정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcC5Yg-3TeJk",
        "outputId": "e9e4627d-9625-4cbe-cb7b-9d3abc902823"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.9/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.9/dist-packages (from fasttext) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (67.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# # Word Representation\n",
        "# # Skipgram model :\n",
        "# skip_model = fasttext.train_unsupervised('./author.txt', model='skipgram')\n",
        "# skip_model.save_model(\"skip_model.bin\")\n",
        "# print(skip_model.words)\n",
        "# # or, cbow model :\n",
        "# cbow_model = fasttext.train_unsupervised('./author.txt', model='cbow')\n",
        "# cbow_model.save_model(\"cbow_model.bin\")\n",
        "# print(cbow_model.words)\n",
        "\n",
        "#  디폴트로 돌리면 0.56\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "file = open('train.txt','w+')\n",
        "for i in train.index:\n",
        "    line = '__label__' + str(train['author'][i])+' '+train['text'][i]\n",
        "    file.write(line+\"\\n\")\n",
        "\n",
        "# Text Classification\n",
        "# input             # training file path (required)\n",
        "# model             # unsupervised fasttext model {cbow, skipgram} [skipgram]\n",
        "# lr                # learning rate [0.05]\n",
        "# dim               # size of word vectors [100]\n",
        "# ws                # size of the context window [5]\n",
        "# epoch             # number of epochs [5]\n",
        "# minCount          # minimal number of word occurences [5]\n",
        "# minn              # min length of char ngram [3]\n",
        "# maxn              # max length of char ngram [6]\n",
        "# neg               # number of negatives sampled [5]\n",
        "# wordNgrams        # max length of word ngram [1]\n",
        "# loss              # loss function {ns, hs, softmax, ova} [ns]\n",
        "# bucket            # number of buckets [2000000]\n",
        "# thread            # number of threads [number of cpus]\n",
        "# lrUpdateRate      # change the rate of updates for the learning rate [100]\n",
        "# t                 # sampling threshold [0.0001]\n",
        "# verbose           # verbose [2]\n",
        "\n",
        "text_clf_model = fasttext.train_supervised('train.txt', epoch=30, minCount=2, maxn=10, verbose=0)\n",
        "print(text_clf_model.words)\n",
        "print(text_clf_model.labels)\n",
        "\n",
        "reuslt = text_clf_model.predict(\"He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.\", k=5)\n",
        "print(reuslt)\n",
        "\n",
        "test = pd.read_csv('test_x.csv')\n",
        "submission = pd.read_csv('sample_submission.csv', index_col=False)"
      ],
      "metadata": {
        "id": "zknjMey2TqC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test.index:\n",
        "    lable, proba = text_clf_model.predict(test['text'][i], k=5)\n",
        "    for la, pr in zip(lable, proba):\n",
        "        if '__label__0' == la:\n",
        "            submission.loc[i, '0'] = pr\n",
        "        elif '__label__1' == la:\n",
        "            submission.loc[i, '1'] = pr\n",
        "        elif '__label__2' == la:\n",
        "            submission.loc[i, '2'] = pr\n",
        "        elif '__label__3' == la:\n",
        "            submission.loc[i, '3'] = pr\n",
        "        elif '__label__4' == la:\n",
        "            submission.loc[i, '4'] = pr\n",
        "    # submission.loc[i, '0'] = proba[lable.loc('__label__0')]\n",
        "    # submission.loc[i, '1'] = proba[4]\n",
        "    # submission.loc[i, '2'] = proba[2]\n",
        "    # submission.loc[i, '3'] = proba[0]\n",
        "    # submission.loc[i, '4'] = proba[3]\n",
        "\n",
        "submission.to_csv('result5_fasttext.csv', index=False)\n",
        "print('end')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9OOzO-3Txqx",
        "outputId": "d8b40d6b-872d-4fb8-9aa4-163f8b3aff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. NLP 전처리**\n",
        "\n",
        "\n",
        "**NLP Preprocessing란?**\n",
        "\n",
        "\n",
        "NLP 전처리는 정해진 정답이 없으며 데이터와 목적에 따라 달라집니다. 이 과정은 주로 모델의 입력인 단어,문장,문서의 vector를 만들기 전에 진행됨\n",
        "\n",
        "<일반적인 NLP 전처리의 과정을 이번 신문기사 분류 대회에 적용>\n",
        "\n",
        "1. 데이터를 불러온 후 각 신문기사들을 눈으로 확인하며 특수문자, 불용어 그리고 문장 구조에 대한 감을 잡습니다.\n",
        "\n",
        "2. 문제의 목적과 분석자의 재량에 따라 불용어를 설정하고 리스트에 저장합니다. 이번 대회에서는 특수 문자와 조사만 제거해도 어느 정도 높은 정확도를 얻을 수 있습니다.\n",
        "\n",
        "3. 불용어 이외의 특수 문자들을 제거합니다. 이번 대회를 위해 저는 정규표현식 패키지(re)를 사용하여 한글과 영어 소문자를 제외한 모든 글자들을 제거하였습니다.\n",
        "\n",
        "4. 형태소 분석을 통해 문장을 형태소 단위의 토큰으로 분리합니다. 이때 내가 설정한 불용어들을 결과로 반환해주는 형태소 분석기를 사용하셔야 합니다. 예를 들어 조사를 불용어로 설정하였는데 조사를 분리해주지 못하는 형태소 분석기는 후보에서 제외하시면 됩니다.\n",
        "\n",
        "5. 형태소 단위의 토큰들을 기반으로 리스트에 저장된 불용어를 제거합니다.\n",
        "\n",
        "\n",
        "\n",
        "## **1) 형태소 분석(Stemming)**\n",
        "\n",
        "형태소 분석: 단어나 문장의 언어적 속성을 파악하는 것\n",
        "\n",
        "- 보통 품사의 태깅(PoS)을 통해 이루어지며 한국어 형태소 분석을 위해 Konlpy 패키지에 있는 다양한 함수를 이용하여 진행 할 수 있습니다.\n",
        "\n",
        "- 형태소 분석을 하는 이유: 주로 **형태소 단위로 의미있는 단어**를 가져가고 싶거나 품사 태깅을 통해 **형용사나 명사를 추출하**고 싶을 때 많이 이용하게 됩니다.\n",
        "\n",
        "- 문장을 띄어쓰기 단위로 분류하여 vectorization을 하게 되면 \"데이콘\"이라는 같은 의미의 토큰 세개가 서로 다른 vector를 갖게 됩니다. 이렇게 되면 모델이 세 단어를 각각 다른 단어로 이해합니다.\n",
        "\n",
        "  - 하지만 형태소 분석을 통해 \"데이콘\"이라는 토큰을 추출한다면 앞의 세 단어는 동일한 vector를 갖게되며 모델이 해당 토큰을 더 잘 학습하는 데 도움이 됩니다.\n",
        "\n",
        "- **형태소 분석은 어쩌면 모델링보다 성능에 더 중요한 영향을 미치는 아주 중요한 과정입니다.**, 시간이 허락한다면 다양한 형태소 분석기를 사용하여 결과를 비교하는 것을 추천드립니다."
      ],
      "metadata": {
        "id": "jjdJOuY6VV9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPn6Y7VLVmR3",
        "outputId": "2b8edd54-fa85-419f-bb72-e0aa74cdb8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-1. Kkma**"
      ],
      "metadata": {
        "id": "Mi7lzHaCV9d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REqGPdzjVu4o",
        "outputId": "4fb30edc-a512-4da0-cf97-bda1234071e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['데이', '콘', '에서', '다양', '하', 'ㄴ', '컴피티션', '을', '즐기', '면서', '실력', '있', '는', '데이터', '분석가', '로', '성장', '하', '세요', '!!', '.']\n",
            " \n",
            "문장에서 명사 추출\n",
            "----------------------\n",
            "['데이', '데이콘', '콘', '다양', '컴피티션', '실력', '데이터', '분석가', '성장']\n",
            " \n",
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('데이', 'NNG'), ('콘', 'NNG'), ('에서', 'JKM'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('컴피티션', 'UN'), ('을', 'JKO'), ('즐기', 'VV'), ('면서', 'ECE'), ('실력', 'NNG'), ('있', 'VV'), ('는', 'ETD'), ('데이터', 'NNG'), ('분석가', 'NNG'), ('로', 'JKM'), ('성장', 'NNG'), ('하', 'XSV'), ('세요', 'EFN'), ('!!', 'SW'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1-2. Okt()**"
      ],
      "metadata": {
        "id": "06QBIq94V_ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "Okt = Okt()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.pos(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZr8iqOeVv5u",
        "outputId": "8abe85e0-6391-48ff-dbf5-75cce37651a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['데', '이콘', '에서', '다양한', '컴피티션', '을', '즐기면서', '실력', '있는', '데이터', '분석', '가로', '성장하세요', '!!.']\n",
            " \n",
            "문장에서 명사 추출\n",
            "----------------------\n",
            "['데', '이콘', '컴피티션', '실력', '데이터', '분석', '가로']\n",
            " \n",
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('데', 'Noun'), ('이콘', 'Noun'), ('에서', 'Josa'), ('다양한', 'Adjective'), ('컴피티션', 'Noun'), ('을', 'Josa'), ('즐기면서', 'Verb'), ('실력', 'Noun'), ('있는', 'Adjective'), ('데이터', 'Noun'), ('분석', 'Noun'), ('가로', 'Noun'), ('성장하세요', 'Adjective'), ('!!.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 외에도 Twitter, Komoran, Hannanum 등의 형태소 분석기(Pos Tagger)들이 존재합니다. 속도와 정확도 면에서 차이가 있으며 주로 Mecab 분석기를 이용합니다. \n",
        "\n",
        "- Mecab: 굉장히 속도가 빠르면서도 좋은 분석 결과를 보여준다.\n",
        "- Komoran: 댓글과 같이 정제되지 않은 글에 대해서 먼저 사용해보면 좋다.(오탈자를 어느정도 고려해준다.)\n",
        "- Kkma: 분석 시간이 오래걸리기 때문에 잘 이용하지 않게 된다.\n",
        "- Okt: 품사 태깅 결과를 Noun, Verb등 알아보기 쉽게 반환해준다.\n",
        "- khaiii: 카카오에서 가장 최근에 공개한 분석기, 성능이 좋다고 알려져 있으며 다양한 실험이 필요하다."
      ],
      "metadata": {
        "id": "81bAtNWGV1BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. 표제어 추출(Lemmatization)**\n",
        "\n",
        "언어학을 전공하지 않은 사람에게 Lemmatization과 stemming은 큰 차이가 없다고 생각합니다.모두 단어의 본 모습을 찾아주는 과정으로서 Konlpy에서 공개한 형태소 분석기들을 이용하면 어느 정도 어간 추출이 가능합니다. 형태소 분석(Pos Tagging)을 stemming이라고 표기한 이유도 이와 같습니다."
      ],
      "metadata": {
        "id": "2K3-ofg1WJVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '성장했었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1yT0pMYV0ft",
        "outputId": "c120af23-020b-4622-a8a5-3b0da95722c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('성장', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '성장하였었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82zqthEUWM6R",
        "outputId": "5eee4c53-93ab-4b47-fde0-30bb3289656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태킹(PoS)\n",
            "----------------------\n",
            "[('성장', 'NNG'), ('하', 'XSV'), ('였', 'EPT'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 불용어 제거(Stopwords removing)**\n",
        "\n",
        "* 불용어 : 문장에서 큰 의미가 없다고 생각되는 단어, 글자들 입니다. \n",
        "  - 불용어는 데이터와 문제에 따라 유동적입니다. \n",
        "  \n",
        "다음과 같은 예시를 생각해 보겠습니다.\n",
        "\n",
        "예시: \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
        "\n",
        "- 예시 문장에서 감성분석을 진행할 때는 \"훌륭한\"과 \"아름다운\"등이 주요 특징으로 사용될 것입니다. 하지만 경우에 따라서는 이러한 형용사들을 제외한 배우들의 연기력과 목소리라는 정보에 집중해야 할 때가 있습니다. 이럴때는 \"훌륭한\"과 \"아름다운\"은 불용어로 정의될 수 있습니다."
      ],
      "metadata": {
        "id": "I_fxl319WPJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "import re\n",
        "tokenizer = Okt()\n",
        "def text_preprocessing(text,tokenizer):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는']\n",
        "    \n",
        "    txt = re.sub('[^가-힣a-z]', ' ', text)\n",
        "    token = tokenizer.morphs(txt)\n",
        "    token = [t for t in token if t not in stopwords]\n",
        "        \n",
        "    return token\n",
        "\n",
        "ex_text = \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
        "example_pre= text_preprocessing(ex_text,tokenizer)"
      ],
      "metadata": {
        "id": "vBk2l4KvWgdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. 영어 소문자와 한글을 제외한 모든 문자를 제거\n",
        "2. Okt를 이용해 형태소 분석\n",
        "3. 형태소 분석기를 거쳐 나온 결과들 중 stopwords 리스트에 포함되지 않는 토큰만 token이라는 리스트에 반환"
      ],
      "metadata": {
        "id": "C4h6xXn4Wh8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_pre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYmJYedyWnER",
        "outputId": "b73ed3f9-d86a-43f6-f843-be8cbeed7788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이번', '에', '새롭게', '개봉', '한', '영화', '의', '배우', '들', '모두', '훌륭한', '연기력', '과', '아름다운', '목소리', '갖고', '있어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. 대회 적용**"
      ],
      "metadata": {
        "id": "toHFggsPWoBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in text_list:\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n"
      ],
      "metadata": {
        "id": "b6siZknUWqG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Vectorization**"
      ],
      "metadata": {
        "id": "fZFoUBgjW3Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization 이란?**\n",
        "\n",
        "NLP를 컴퓨터가 이해할 수 있게 수치로 바꾸는 것을 말합니다.이 때 벡터로 변환된 고유의 토큰들이 모인 집합을 vocabulary 하며 vocabulary가 크면 클수록 학습이 오래 걸리게 됩니다. "
      ],
      "metadata": {
        "id": "dADVuIS4W423"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단한 전처리와 okt를 활용하여 나누어보겠습니다.\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "Okt = Okt()\n",
        "\n",
        "sentences = ['자연어 처리는 정말 정말 즐거워.', '즐거운 자연어 처리 다같이 해보자.']\n",
        "tokens = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence = re.sub('[^가-힣a-z]', ' ', sentence) #간단한 전처리\n",
        "    token = (Okt.morphs(sentence)) #형태소 분석기를 이용햔 토큰 나누기\n",
        "    tokens.append(' '.join(token))\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgreGrEfW_mc",
        "outputId": "9cd8439a-f52a-4bcf-b2b9-199c11b97343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위로 문장 분리\n",
            "----------------------\n",
            "['자연어 처리 는 정말 정말 즐거워', '즐거운 자연어 처리 다 같이 해보자']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) One Hot Encoding**\n",
        "\n",
        ": 해당 단어가 존재하면 1, 그렇지 않으면 모두 0으로 표시되는 기법입니다. \n",
        "\n",
        "keras를 이용하면 쉽게 구현이 가능하며 texts_to_sequences를 활용해 각 토큰에 고유한 정수를 부여한 후 to_categorical을 활용하면 간단하게 구현이 가능합니다."
      ],
      "metadata": {
        "id": "iijjpIUzXIpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(tokens)\n",
        "print(\"각 토큰에게 고유의 정수 부여\")\n",
        "print(\"----------------------\")\n",
        "print(t.word_index) \n",
        "print(\" \")\n",
        "\n",
        "s1=t.texts_to_sequences(tokens)[0] \n",
        "print(\"부여된 정수로 표시된 문장1\")\n",
        "print(\"----------------------\")\n",
        "print(s1)\n",
        "print(\" \")\n",
        "\n",
        "s2=t.texts_to_sequences(tokens)[1]\n",
        "print(\"부여된 정수로 표시된 문장2\")\n",
        "print(\"----------------------\")\n",
        "print(s2)\n",
        "print(\" \")\n",
        "\n",
        "s1_one_hot = to_categorical(s1)\n",
        "print(\"문장1의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s1_one_hot)\n",
        "print(\" \")\n",
        "\n",
        "s2_one_hot = to_categorical(s2)\n",
        "print(\"문장2의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s2_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rppG3mk-XSbU",
        "outputId": "4feca3da-66be-4ee4-8f90-e80865b383c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "각 토큰에게 고유의 정수 부여\n",
            "----------------------\n",
            "{'자연어': 1, '처리': 2, '정말': 3, '는': 4, '즐거워': 5, '즐거운': 6, '다': 7, '같이': 8, '해보자': 9}\n",
            " \n",
            "부여된 정수로 표시된 문장1\n",
            "----------------------\n",
            "[1, 2, 4, 3, 3, 5]\n",
            " \n",
            "부여된 정수로 표시된 문장2\n",
            "----------------------\n",
            "[6, 1, 2, 7, 8, 9]\n",
            " \n",
            "문장1의 one-hot-encoding\n",
            "----------------------\n",
            "[[0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]]\n",
            " \n",
            "문장2의 one-hot-encoding\n",
            "----------------------\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 하지만 이 방식은 vocabulary 크기가 커짐에 따라 많은 공간을 차지하게 되고 벡터가 굉장히 sparse해지기 때문에 모델에게 좋은 특성을 알려주지 못하는 경우가 대부분입니다."
      ],
      "metadata": {
        "id": "BSZq_A5qXWxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2) Count vectorization**\n",
        "\n",
        "이 방식은 vocabulary를 활용하여 각 문장이 갖고 있는 토큰의 count를 기반으로 문장을 vectorization 해줍니다. "
      ],
      "metadata": {
        "id": "GtU3vuacXY0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(tokens) #여러 개의 문장을 넣어줘야 작동합니다!!\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(vectors.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJpInolbXUUj",
        "outputId": "e1e43c5e-a56b-4902-8e9b-82db8465c433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['같이' '자연어' '정말' '즐거운' '즐거워' '처리' '해보자']\n",
            "[[0 1 2 0 1 1 0]\n",
            " [1 1 0 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 벡터화 결과 각 문장은 vocabulary의 인덱스를 기준으로 카운트가 정수로 표시된 것을 알 수 있습니다.\n",
        "  - 아쉬운 점은 sklearn의 CountVectorizer는 한 글자는 자동으로 제거해주네요\n",
        "\n",
        "  \n",
        "*   또한, 이 부분에서 형태소 분석기의 중요성에 대해서 알 수 있습니다. 즐거운과 즐거워는 같은 의미를 갖는 토큰이지만 okt는 이를 구분해주지 못해서 다른 토큰으로 분리가 되었습니다. 이는 모델에서 같은 의미의 토큰을 다르게 학습할 수 있음을 의미합니다."
      ],
      "metadata": {
        "id": "XDjWScYnXgWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3) TfIdf**\n",
        "\n",
        "1. 단어가 몇번 등장 했는지에 대한 정보\n",
        "2. 어떤 단어가 언급된 문서의 수가 적다면 그 단어는 문서를 분류하는데 있어서 중요한 단어\n",
        "\n",
        "즉, 등장 횟수도 많고 문서 분별력 있는 단어들을 점수화하여 벡터화를 한 것이 TfIdf 기법입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCQfoPlhXlW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=0)\n",
        "tfidf_vectorizer = tfidf.fit_transform(tokens) \n",
        "\n",
        "#tf-idf dictionary    \n",
        "tfidf_dict = tfidf.get_feature_names_out()\n",
        "print(tfidf_dict)\n",
        "print(tfidf_vectorizer.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FivkHbzkXgAw",
        "outputId": "97395637-7739-4495-f10a-7c638a25503c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['같이' '자연어' '정말' '즐거운' '즐거워' '처리' '해보자']\n",
            "[[0.         0.29017021 0.81564821 0.         0.4078241  0.29017021\n",
            "  0.        ]\n",
            " [0.49922133 0.35520009 0.         0.49922133 0.         0.35520009\n",
            "  0.49922133]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4) Padding**\n",
        "- 우리가 사용하던 모델들은 DataFrame 형식의 row별 동일한 colum수를 갖는데 NLP에서는 row별(문장별) colum(토큰의 개수)가 같지 않아도 되는건가? \n",
        "- 가변 길이의 문장들을 입력으로 넣어도 된다고? 하는 의문점\n",
        "  - 가변 길이의 입력을 받는 모형들이 존재하지만 아쉽게도 기본적으로는 문장의 길이를 동일하게 맞춰주어야 합니다. \n",
        "  - 문장의 길이를 맞춰주기 위해 부족한 길이만큼 0을 채워넣게 되는데 우리는 이것을 **Padding**라고 부릅니다."
      ],
      "metadata": {
        "id": "fyz2mFPJXt2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5) 대회 적용**"
      ],
      "metadata": {
        "id": "B161G15qCsmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU3gctOnX1jq",
        "outputId": "e7c3f5e2-bc6b-48e8-a39d-e0a0d57a4905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  42331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 바로 모델에 넣고 훈련을 시작하면 되는 걸까요?\n",
        "\n",
        "아니요, 아직입니다. 이번에 살펴본 vectorization 방법들은 치명적인 단점을 갖고 있습니다.\n",
        "\n",
        "이 문제를 해결해 주는 것이 바로 Embedding 입니다.\n",
        "\n",
        "모델링 전까지 험난한 여정의 NLP 전처리!! Embedding으로 이어집니다."
      ],
      "metadata": {
        "id": "qN79gKl8X5bv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqxQLtnkAa1CP2Ifm+3rUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}